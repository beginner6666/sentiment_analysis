{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, SpatialDropout1D, Input, Bidirectional,Dropout, Activation, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Train_DataSet.csv')\n",
    "train_label = pd.read_csv('Train_DataSet_Label.csv')\n",
    "train = pd.merge(train_data, train_label, how='left', on='id')\n",
    "train = train[(train.label.notnull()) & (train.content.notnull())]\n",
    "test = pd.read_csv('Test_DataSet.csv')\n",
    "\n",
    "train['title'] = train['title'].fillna('')\n",
    "train['content'] = train['content'].fillna('')\n",
    "test['title'] = test['title'].fillna('')\n",
    "test['content'] = test['content'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def filter(text):\n",
    "    text = re.sub(\"[A-Za-z0-9\\!\\=\\？\\%\\[\\]\\,\\（\\）\\>\\<:&lt;\\/#\\. -----\\_]\", \"\", text)\n",
    "    text = text.replace('图片', '')\n",
    "    text = text.replace('\\xa0', '') # 删除nbsp\n",
    "    # new\n",
    "    r1 =  \"\\\\【.*?】+|\\\\《.*?》+|\\\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\\\\\，。=？、：“”‘’￥……（）《》【】]\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    text = re.sub(cleanr, ' ', text)        #去除html标签\n",
    "    text = re.sub(r1,'',text)\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Possible set difference at position 43\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Possible set difference at position 45\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Possible set difference at position 46\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def clean_text(data):\n",
    "    data['title'] = data['title'].apply(lambda x: filter(x))\n",
    "    data['content'] = data['content'].apply(lambda x: filter(x))\n",
    "    return data\n",
    "train = clean_text(train)\n",
    "test = clean_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "stop_words = pd.read_table('stop.txt', header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import string\n",
    "table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "def cut_text(sentence):\n",
    "    tokens = list(jieba.cut(sentence))\n",
    "    # 去除停用词\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "#     # 去除英文标点\n",
    "#     tokens = [w.translate(table) for w in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.408 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "train_title = [cut_text(sent) for sent in train.title.values]\n",
    "train_content = [cut_text(sent) for sent in train.content.values]\n",
    "test_title = [cut_text(sent) for sent in test.title.values]\n",
    "test_content = [cut_text(sent) for sent in test.content.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0dcd681d2713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_title\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shap'"
     ]
    }
   ],
   "source": [
    "print(type(train_title))\n",
    "train_title.shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7266\n",
      "7266\n"
     ]
    }
   ],
   "source": [
    "print(len(train_title))\n",
    "print(len(train_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['问责', '领导', '上', '黄镇', '党委书记', '张涛', '宣国', '才', '真能', '一手遮天', '几天', '看', '有人', '举报', '施', '某某', '贴子', '举报人', '联系', '证实', '宣某', '当天', '中午', '请', '举报人', '枪手', '喝酒', '后', '晚上', '才', '发', '贴子', '本人', '不去', '讨论', '前', '二天', '举报', '相信', '总归', '会', '说法', '今天', '看施', '全军', '年月日', '实名', '举报', '上', '黄镇', '宣国', '才', '贴子', '仍', '锁定', '禁止', '评论', '已经', '正好', '整年', '施', '全军', '实名', '举报', '天后', '上', '黄镇', '党委政府', '回复', '如下', '图', '一年', '贴子', '再次', '网友', '顶', '起来', '后', '才', '发现', '施某', '几天', '前', '回复', '网友', '处理结果', '竟', '如下', '图现', '责问', '张涛', '书记', '宣国', '才', '举报', '问题', '答复', '宣国', '才', '举报', '后', '立刻', '免', '村', '书记', '职务', '安排', '城管', '队', '吃', '空响', '却', '天天', '水泥厂', '上班', '赚', '黑钱', '几个', '月', '水泥', '每吨', '近元', '纯利润', '还', '供不应求', '宣国', '才', '还清', '上', '黄', '政府', '担保', '借给', '宣国', '才', '代付', '振', '东厂', '工资', '社保', '钱', '了解', '宣国', '才', '占', '企业', '经营', '欠税', '万元', '欠', '社保', '万元', '应该', '还', '欠', '职工工资', '几十万', '上', '黄', '政府', '打算', '宣国', '才', '担保', '归还', '厂', '合法', '会计', '老板', '判刑', '四到', '六年', '现在', '服刑', '厂子', '宣国', '才', '强占', '宣国', '才', '每天', '赚', '多万', '净利润', '却', '对外', '宣称', '天天', '亏本', '老板', '刑满', '回厂', '宣国', '才', '咱厂', '天天', '亏', '可能', '亏', '几千万元', '几个', '亿张', '涛', '书记', '承担', '上', '黄', '政府', '承担', '当初', '亲自', '厂', '交给', '宣国', '才', '生产', '希望', '徐', '市长', '看到', '本贴', '后', '批示', '批示', '违建', '民生问题', '关注', '一下', '水泥厂', '将来', '请', '徐', '市长', '抽', '日理万机', '之空', '亲自', '约', '谈', '一下', '当事人', '特别', '那位', '施', '站长', '千万', '不能', '听取', '一面之辞']\n",
      "问责领导上黄镇党委书记张涛宣国才真能一手遮天几天看有人举报施某某贴子举报人联系证实宣某当天中午请举报人枪手喝酒后晚上才发贴子本人不去讨论前二天举报相信总归会说法今天看施全军年月日实名举报上黄镇宣国才贴子仍锁定禁止评论已经正好整年施全军实名举报天后上黄镇党委政府回复如下图一年贴子再次网友顶起来后才发现施某几天前回复网友处理结果竟如下图现责问张涛书记宣国才举报问题答复宣国才举报后立刻免村书记职务安排城管队吃空响却天天水泥厂上班赚黑钱几个月水泥每吨近元纯利润还供不应求宣国才还清上黄政府担保借给宣国才代付振东厂工资社保钱了解宣国才占企业经营欠税万元欠社保万元应该还欠职工工资几十万上黄政府打算宣国才担保归还厂合法会计老板判刑四到六年现在服刑厂子宣国才强占宣国才每天赚多万净利润却对外宣称天天亏本老板刑满回厂宣国才咱厂天天亏可能亏几千万元几个亿张涛书记承担上黄政府承担当初亲自厂交给宣国才生产希望徐市长看到本贴后批示批示违建民生问题关注一下水泥厂将来请徐市长抽日理万机之空亲自约谈一下当事人特别那位施站长千万不能听取一面之辞\n"
     ]
    }
   ],
   "source": [
    "with open ('in.tsv','w',encoding='utf-8') as file:\n",
    "    for i in range(6):\n",
    "        file.write(\"\".join(str(s) for s in train_title[i]+train_content[i]))\n",
    "\n",
    "print(train_title[0]+train_content[0])\n",
    "\n",
    "str4 = \"\".join(train_title[0]+train_content[0])\n",
    "print(str4)\n",
    "\n",
    "\n",
    "\n",
    "#print(str)\n",
    "#print(len(train_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc = train_title + train_content + test_title + test_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'describe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-856b6244b04d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'describe'"
     ]
    }
   ],
   "source": [
    "all_doc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'profile_report'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-5ae36a8ba67d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_profiling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mall_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'profile_report'"
     ]
    }
   ],
   "source": [
    "import pandas_profiling  \n",
    "\n",
    "all_doc.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2a48d7bba74f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'all_doc' is not defined"
     ]
    }
   ],
   "source": [
    "print(all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import time\n",
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "\n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "# model_word2vec = gensim.models.Word2Vec.load('final_word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0min 16s\n"
     ]
    }
   ],
   "source": [
    "model_word2vec = gensim.models.Word2Vec(min_count=1, \n",
    "                                        window=5, \n",
    "                                        size=256,\n",
    "                                        workers=4,\n",
    "                                        batch_words=1000)\n",
    "since = time.time()\n",
    "model_word2vec.build_vocab(all_doc, progress_per=2000)\n",
    "time_elapsed = time.time() - since\n",
    "print('Time to build vocab: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 4072212.50, time: 0min 13s\n",
      "Better model. Best loss: 4072212.50\n",
      "Model ./final_word2vec_model save done!\n",
      "Epoch 2, loss: 2683699.50, time: 0min 12s\n",
      "Better model. Best loss: 2683699.50\n",
      "Model ./final_word2vec_model save done!\n",
      "Epoch 3, loss: 2279267.00, time: 0min 12s\n",
      "Better model. Best loss: 2279267.00\n",
      "Model ./final_word2vec_model save done!\n",
      "Epoch 4, loss: 1931114.00, time: 0min 12s\n",
      "Better model. Best loss: 1931114.00\n",
      "Model ./final_word2vec_model save done!\n",
      "Epoch 5, loss: 1827277.00, time: 0min 12s\n",
      "Better model. Best loss: 1827277.00\n",
      "Model ./final_word2vec_model save done!\n",
      "Time to train: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "model_word2vec.train(all_doc, total_examples=model_word2vec.corpus_count, \n",
    "                        epochs=5, compute_loss=True, report_delay=60*10,\n",
    "                        callbacks=[EpochSaver('./final_word2vec_model')])\n",
    "time_elapsed = time.time() - since\n",
    "print('Time to train: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_title + test_title)\n",
    "#print(tokenizer)\n",
    "# tokenizer.fit_on_texts(train_content + test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/32058 [00:00<?, ?it/s]E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 32058/32058 [00:00<00:00, 73428.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# 转化成词向量矩阵，利用新的word2vec模型\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(vocab_size)\n",
    "error_count=0\n",
    "embedding_matrix = np.zeros((vocab_size + 1, 256))\n",
    "for word, i in tqdm(tokenizer.word_index.items()):\n",
    "    if word in model_word2vec:\n",
    "        embedding_matrix[i] = model_word2vec.wv[word]\n",
    "    else:\n",
    "        error_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(train_title)\n",
    "traintitle = pad_sequences(sequence, maxlen=30)\n",
    "sequence = tokenizer.texts_to_sequences(test_title)\n",
    "testtitle = pad_sequences(sequence, maxlen=30)\n",
    "# sequence = tokenizer.texts_to_sequences(train_content)\n",
    "# traincontent = pad_sequences(sequence, maxlen=512)\n",
    "# sequence = tokenizer.texts_to_sequences(test_content)\n",
    "# testcontent = pad_sequences(sequence, maxlen=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def metric_F1score(y_true,y_pred):    \n",
    "    TP=tf.reduce_sum(y_true*tf.round(y_pred))\n",
    "    TN=tf.reduce_sum((1-y_true)*(1-tf.round(y_pred)))\n",
    "    FP=tf.reduce_sum((1-y_true)*tf.round(y_pred))\n",
    "    FN=tf.reduce_sum(y_true*(1-tf.round(y_pred)))\n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/(TP+FN)\n",
    "    F1score=2*precision*recall/(precision+recall)\n",
    "    return F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 256)           8207104   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               164352    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 8,372,779\n",
      "Trainable params: 8,372,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n",
    "                    output_dim=256, \n",
    "                    input_length=30, \n",
    "                    weights=[embedding_matrix]))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.1)))\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "optimizer = keras.optimizers.Adam(lr=0.005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "#                                             patience=3, \n",
    "#                                             verbose=1, \n",
    "#                                             factor=0.5, \n",
    "#                                             min_lr=0.00001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n",
    "                    output_dim=256, \n",
    "                    input_length=30, \n",
    "                    weights=[embedding_matrix]))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM\n",
    "\n",
    "from attention import Attention\n",
    "\n",
    "class TextAttBiRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, \n",
    "                              input_length=self.maxlen, weights=[embedding_matrix])(input)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(embedding)  # LSTM or GRU\n",
    "        x = Attention(self.maxlen)(x)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model\n",
    "model = TextAttBiRNN(maxlen=30, max_features=len(tokenizer.word_index) + 1,\n",
    "                    embedding_dims=256, class_num=3, last_activation='softmax').get_model()\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textCnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    "class TextCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        # Embedding part can try multichannel as same as origin paper\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen,\n",
    "                              weights=[embedding_matrix])(input)\n",
    "        convs = []\n",
    "        for kernel_size in [3, 4, 5]:\n",
    "            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "            c = GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model\n",
    "    \n",
    "model = TextCNN(maxlen=30, max_features=len(tokenizer.word_index) + 1,\n",
    "                    embedding_dims=256, class_num=3, last_activation='softmax').get_model()\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy',metric_F1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 30, 256)      8207104     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 28, 128)      98432       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 27, 128)      131200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 26, 128)      163968      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            1155        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,601,859\n",
      "Trainable params: 8,601,859\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train['label'].astype(int)\n",
    "# labels = to_categorical(label) \n",
    "# train_X, val_X, train_Y, val_Y = train_test_split(traintitle, label, shuffle=True, test_size=0.2,\n",
    "#                                                     random_state=2019)\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(traintitle, label, shuffle=True, test_size=0.2,\n",
    "                                                    random_state=2019)\n",
    "train_Y = to_categorical(train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5812/5812 [==============================] - ETA: 2:11 - loss: 1.5719 - acc: 0.2812 - metric_F1score: 0.22 - ETA: 1:10 - loss: 1.3251 - acc: 0.4258 - metric_F1score: 0.39 - ETA: 49s - loss: 1.1979 - acc: 0.5104 - metric_F1score: 0.4901 - ETA: 39s - loss: 1.1860 - acc: 0.5586 - metric_F1score: 0.540 - ETA: 32s - loss: 1.2265 - acc: 0.5641 - metric_F1score: 0.547 - ETA: 28s - loss: 1.2120 - acc: 0.5794 - metric_F1score: 0.563 - ETA: 25s - loss: 1.1842 - acc: 0.5770 - metric_F1score: 0.561 - ETA: 22s - loss: 1.1536 - acc: 0.5879 - metric_F1score: 0.574 - ETA: 20s - loss: 1.1458 - acc: 0.5955 - metric_F1score: 0.582 - ETA: 19s - loss: 1.1412 - acc: 0.5969 - metric_F1score: 0.584 - ETA: 17s - loss: 1.1151 - acc: 0.6037 - metric_F1score: 0.591 - ETA: 16s - loss: 1.0842 - acc: 0.6094 - metric_F1score: 0.596 - ETA: 15s - loss: 1.0698 - acc: 0.6148 - metric_F1score: 0.602 - ETA: 14s - loss: 1.0501 - acc: 0.6217 - metric_F1score: 0.609 - ETA: 13s - loss: 1.0278 - acc: 0.6297 - metric_F1score: 0.618 - ETA: 13s - loss: 1.0220 - acc: 0.6348 - metric_F1score: 0.623 - ETA: 12s - loss: 1.0369 - acc: 0.6337 - metric_F1score: 0.622 - ETA: 11s - loss: 1.0229 - acc: 0.6380 - metric_F1score: 0.626 - ETA: 11s - loss: 1.0074 - acc: 0.6386 - metric_F1score: 0.628 - ETA: 10s - loss: 0.9931 - acc: 0.6441 - metric_F1score: 0.631 - ETA: 9s - loss: 0.9893 - acc: 0.6425 - metric_F1score: 0.631 - ETA: 9s - loss: 0.9759 - acc: 0.6442 - metric_F1score: 0.63 - ETA: 8s - loss: 0.9673 - acc: 0.6450 - metric_F1score: 0.63 - ETA: 8s - loss: 0.9504 - acc: 0.6504 - metric_F1score: 0.63 - ETA: 7s - loss: 0.9408 - acc: 0.6541 - metric_F1score: 0.64 - ETA: 7s - loss: 0.9325 - acc: 0.6572 - metric_F1score: 0.64 - ETA: 6s - loss: 0.9285 - acc: 0.6606 - metric_F1score: 0.64 - ETA: 6s - loss: 0.9174 - acc: 0.6635 - metric_F1score: 0.65 - ETA: 6s - loss: 0.9171 - acc: 0.6643 - metric_F1score: 0.65 - ETA: 5s - loss: 0.9120 - acc: 0.6656 - metric_F1score: 0.65 - ETA: 5s - loss: 0.9137 - acc: 0.6651 - metric_F1score: 0.65 - ETA: 4s - loss: 0.9107 - acc: 0.6655 - metric_F1score: 0.65 - ETA: 4s - loss: 0.9049 - acc: 0.6669 - metric_F1score: 0.65 - ETA: 4s - loss: 0.8981 - acc: 0.6691 - metric_F1score: 0.65 - ETA: 3s - loss: 0.8888 - acc: 0.6723 - metric_F1score: 0.66 - ETA: 3s - loss: 0.8837 - acc: 0.6738 - metric_F1score: 0.66 - ETA: 2s - loss: 0.8789 - acc: 0.6740 - metric_F1score: 0.66 - ETA: 2s - loss: 0.8750 - acc: 0.6754 - metric_F1score: 0.66 - ETA: 2s - loss: 0.8706 - acc: 0.6769 - metric_F1score: 0.66 - ETA: 1s - loss: 0.8681 - acc: 0.6777 - metric_F1score: 0.66 - ETA: 1s - loss: 0.8613 - acc: 0.6795 - metric_F1score: 0.66 - ETA: 1s - loss: 0.8556 - acc: 0.6814 - metric_F1score: 0.67 - ETA: 0s - loss: 0.8528 - acc: 0.6810 - metric_F1score: 0.67 - ETA: 0s - loss: 0.8481 - acc: 0.6832 - metric_F1score: 0.67 - ETA: 0s - loss: 0.8444 - acc: 0.6844 - metric_F1score: 0.67 - 15s 3ms/step - loss: 0.8423 - acc: 0.6848 - metric_F1score: 0.6742\n",
      "Epoch 2/10\n",
      "5812/5812 [==============================] - ETA: 12s - loss: 0.3546 - acc: 0.8906 - metric_F1score: 0.885 - ETA: 12s - loss: 0.4108 - acc: 0.8438 - metric_F1score: 0.837 - ETA: 11s - loss: 0.3960 - acc: 0.8464 - metric_F1score: 0.840 - ETA: 11s - loss: 0.3961 - acc: 0.8457 - metric_F1score: 0.840 - ETA: 11s - loss: 0.4015 - acc: 0.8438 - metric_F1score: 0.841 - ETA: 11s - loss: 0.4053 - acc: 0.8424 - metric_F1score: 0.839 - ETA: 10s - loss: 0.4188 - acc: 0.8337 - metric_F1score: 0.832 - ETA: 10s - loss: 0.4181 - acc: 0.8320 - metric_F1score: 0.831 - ETA: 10s - loss: 0.4084 - acc: 0.8359 - metric_F1score: 0.835 - ETA: 9s - loss: 0.4053 - acc: 0.8414 - metric_F1score: 0.837 - ETA: 9s - loss: 0.4094 - acc: 0.8409 - metric_F1score: 0.83 - ETA: 9s - loss: 0.4103 - acc: 0.8392 - metric_F1score: 0.83 - ETA: 9s - loss: 0.4105 - acc: 0.8383 - metric_F1score: 0.83 - ETA: 8s - loss: 0.4175 - acc: 0.8326 - metric_F1score: 0.82 - ETA: 8s - loss: 0.4178 - acc: 0.8297 - metric_F1score: 0.82 - ETA: 8s - loss: 0.4112 - acc: 0.8340 - metric_F1score: 0.82 - ETA: 8s - loss: 0.4071 - acc: 0.8369 - metric_F1score: 0.83 - ETA: 7s - loss: 0.4088 - acc: 0.8355 - metric_F1score: 0.83 - ETA: 7s - loss: 0.4140 - acc: 0.8314 - metric_F1score: 0.82 - ETA: 7s - loss: 0.4145 - acc: 0.8324 - metric_F1score: 0.82 - ETA: 6s - loss: 0.4107 - acc: 0.8359 - metric_F1score: 0.83 - ETA: 6s - loss: 0.4089 - acc: 0.8374 - metric_F1score: 0.83 - ETA: 6s - loss: 0.4116 - acc: 0.8346 - metric_F1score: 0.83 - ETA: 6s - loss: 0.4134 - acc: 0.8327 - metric_F1score: 0.82 - ETA: 5s - loss: 0.4099 - acc: 0.8350 - metric_F1score: 0.83 - ETA: 5s - loss: 0.4103 - acc: 0.8359 - metric_F1score: 0.83 - ETA: 5s - loss: 0.4093 - acc: 0.8362 - metric_F1score: 0.83 - ETA: 4s - loss: 0.4103 - acc: 0.8371 - metric_F1score: 0.83 - ETA: 4s - loss: 0.4095 - acc: 0.8381 - metric_F1score: 0.83 - ETA: 4s - loss: 0.4111 - acc: 0.8367 - metric_F1score: 0.83 - ETA: 4s - loss: 0.4110 - acc: 0.8372 - metric_F1score: 0.83 - ETA: 3s - loss: 0.4124 - acc: 0.8367 - metric_F1score: 0.83 - ETA: 3s - loss: 0.4149 - acc: 0.8350 - metric_F1score: 0.83 - ETA: 3s - loss: 0.4227 - acc: 0.8318 - metric_F1score: 0.82 - ETA: 2s - loss: 0.4230 - acc: 0.8317 - metric_F1score: 0.82 - ETA: 2s - loss: 0.4235 - acc: 0.8320 - metric_F1score: 0.82 - ETA: 2s - loss: 0.4228 - acc: 0.8330 - metric_F1score: 0.82 - ETA: 2s - loss: 0.4232 - acc: 0.8329 - metric_F1score: 0.82 - ETA: 1s - loss: 0.4237 - acc: 0.8333 - metric_F1score: 0.82 - ETA: 1s - loss: 0.4248 - acc: 0.8328 - metric_F1score: 0.82 - ETA: 1s - loss: 0.4260 - acc: 0.8319 - metric_F1score: 0.82 - ETA: 0s - loss: 0.4255 - acc: 0.8322 - metric_F1score: 0.82 - ETA: 0s - loss: 0.4266 - acc: 0.8319 - metric_F1score: 0.82 - ETA: 0s - loss: 0.4272 - acc: 0.8317 - metric_F1score: 0.82 - ETA: 0s - loss: 0.4272 - acc: 0.8313 - metric_F1score: 0.82 - 13s 2ms/step - loss: 0.4288 - acc: 0.8305 - metric_F1score: 0.8274\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5812/5812 [==============================] - ETA: 11s - loss: 0.3002 - acc: 0.9062 - metric_F1score: 0.905 - ETA: 12s - loss: 0.2673 - acc: 0.9375 - metric_F1score: 0.931 - ETA: 12s - loss: 0.2734 - acc: 0.9401 - metric_F1score: 0.930 - ETA: 11s - loss: 0.2810 - acc: 0.9258 - metric_F1score: 0.916 - ETA: 11s - loss: 0.2914 - acc: 0.9156 - metric_F1score: 0.909 - ETA: 11s - loss: 0.2892 - acc: 0.9102 - metric_F1score: 0.905 - ETA: 10s - loss: 0.2860 - acc: 0.9129 - metric_F1score: 0.908 - ETA: 10s - loss: 0.2776 - acc: 0.9150 - metric_F1score: 0.911 - ETA: 10s - loss: 0.2929 - acc: 0.9115 - metric_F1score: 0.908 - ETA: 10s - loss: 0.2976 - acc: 0.9062 - metric_F1score: 0.903 - ETA: 9s - loss: 0.2964 - acc: 0.9048 - metric_F1score: 0.903 - ETA: 9s - loss: 0.2888 - acc: 0.9076 - metric_F1score: 0.90 - ETA: 9s - loss: 0.2872 - acc: 0.9087 - metric_F1score: 0.90 - ETA: 8s - loss: 0.2839 - acc: 0.9102 - metric_F1score: 0.90 - ETA: 8s - loss: 0.2830 - acc: 0.9120 - metric_F1score: 0.91 - ETA: 8s - loss: 0.2776 - acc: 0.9146 - metric_F1score: 0.91 - ETA: 8s - loss: 0.2754 - acc: 0.9145 - metric_F1score: 0.91 - ETA: 7s - loss: 0.2752 - acc: 0.9149 - metric_F1score: 0.91 - ETA: 7s - loss: 0.2731 - acc: 0.9141 - metric_F1score: 0.91 - ETA: 7s - loss: 0.2694 - acc: 0.9160 - metric_F1score: 0.91 - ETA: 6s - loss: 0.2682 - acc: 0.9159 - metric_F1score: 0.91 - ETA: 6s - loss: 0.2696 - acc: 0.9141 - metric_F1score: 0.91 - ETA: 6s - loss: 0.2674 - acc: 0.9147 - metric_F1score: 0.91 - ETA: 5s - loss: 0.2686 - acc: 0.9141 - metric_F1score: 0.91 - ETA: 5s - loss: 0.2663 - acc: 0.9147 - metric_F1score: 0.91 - ETA: 5s - loss: 0.2695 - acc: 0.9135 - metric_F1score: 0.91 - ETA: 5s - loss: 0.2697 - acc: 0.9129 - metric_F1score: 0.91 - ETA: 4s - loss: 0.2688 - acc: 0.9135 - metric_F1score: 0.91 - ETA: 4s - loss: 0.2703 - acc: 0.9122 - metric_F1score: 0.91 - ETA: 4s - loss: 0.2720 - acc: 0.9109 - metric_F1score: 0.91 - ETA: 4s - loss: 0.2711 - acc: 0.9118 - metric_F1score: 0.91 - ETA: 3s - loss: 0.2746 - acc: 0.9104 - metric_F1score: 0.90 - ETA: 3s - loss: 0.2746 - acc: 0.9103 - metric_F1score: 0.90 - ETA: 3s - loss: 0.2749 - acc: 0.9102 - metric_F1score: 0.90 - ETA: 2s - loss: 0.2735 - acc: 0.9112 - metric_F1score: 0.91 - ETA: 2s - loss: 0.2748 - acc: 0.9099 - metric_F1score: 0.90 - ETA: 2s - loss: 0.2747 - acc: 0.9096 - metric_F1score: 0.90 - ETA: 2s - loss: 0.2753 - acc: 0.9089 - metric_F1score: 0.90 - ETA: 1s - loss: 0.2760 - acc: 0.9079 - metric_F1score: 0.90 - ETA: 1s - loss: 0.2767 - acc: 0.9082 - metric_F1score: 0.90 - ETA: 1s - loss: 0.2768 - acc: 0.9082 - metric_F1score: 0.90 - ETA: 0s - loss: 0.2771 - acc: 0.9079 - metric_F1score: 0.90 - ETA: 0s - loss: 0.2793 - acc: 0.9070 - metric_F1score: 0.90 - ETA: 0s - loss: 0.2773 - acc: 0.9084 - metric_F1score: 0.90 - ETA: 0s - loss: 0.2772 - acc: 0.9076 - metric_F1score: 0.90 - 13s 2ms/step - loss: 0.2778 - acc: 0.9074 - metric_F1score: 0.9073\n",
      "Epoch 4/10\n",
      "5812/5812 [==============================] - ETA: 12s - loss: 0.2324 - acc: 0.9297 - metric_F1score: 0.912 - ETA: 12s - loss: 0.2124 - acc: 0.9492 - metric_F1score: 0.938 - ETA: 11s - loss: 0.1980 - acc: 0.9635 - metric_F1score: 0.955 - ETA: 11s - loss: 0.1935 - acc: 0.9629 - metric_F1score: 0.956 - ETA: 11s - loss: 0.1805 - acc: 0.9688 - metric_F1score: 0.962 - ETA: 11s - loss: 0.1836 - acc: 0.9688 - metric_F1score: 0.963 - ETA: 11s - loss: 0.1803 - acc: 0.9710 - metric_F1score: 0.965 - ETA: 10s - loss: 0.1761 - acc: 0.9717 - metric_F1score: 0.966 - ETA: 10s - loss: 0.1785 - acc: 0.9696 - metric_F1score: 0.964 - ETA: 10s - loss: 0.1770 - acc: 0.9695 - metric_F1score: 0.964 - ETA: 9s - loss: 0.1738 - acc: 0.9695 - metric_F1score: 0.964 - ETA: 9s - loss: 0.1707 - acc: 0.9701 - metric_F1score: 0.96 - ETA: 9s - loss: 0.1720 - acc: 0.9688 - metric_F1score: 0.96 - ETA: 9s - loss: 0.1750 - acc: 0.9671 - metric_F1score: 0.96 - ETA: 8s - loss: 0.1754 - acc: 0.9672 - metric_F1score: 0.96 - ETA: 8s - loss: 0.1749 - acc: 0.9668 - metric_F1score: 0.96 - ETA: 8s - loss: 0.1738 - acc: 0.9669 - metric_F1score: 0.96 - ETA: 7s - loss: 0.1725 - acc: 0.9674 - metric_F1score: 0.96 - ETA: 7s - loss: 0.1719 - acc: 0.9675 - metric_F1score: 0.96 - ETA: 7s - loss: 0.1728 - acc: 0.9668 - metric_F1score: 0.96 - ETA: 7s - loss: 0.1719 - acc: 0.9658 - metric_F1score: 0.96 - ETA: 6s - loss: 0.1717 - acc: 0.9648 - metric_F1score: 0.96 - ETA: 6s - loss: 0.1717 - acc: 0.9650 - metric_F1score: 0.96 - ETA: 6s - loss: 0.1700 - acc: 0.9658 - metric_F1score: 0.96 - ETA: 5s - loss: 0.1715 - acc: 0.9647 - metric_F1score: 0.96 - ETA: 5s - loss: 0.1702 - acc: 0.9648 - metric_F1score: 0.96 - ETA: 5s - loss: 0.1695 - acc: 0.9653 - metric_F1score: 0.96 - ETA: 4s - loss: 0.1695 - acc: 0.9651 - metric_F1score: 0.96 - ETA: 4s - loss: 0.1693 - acc: 0.9647 - metric_F1score: 0.96 - ETA: 4s - loss: 0.1692 - acc: 0.9648 - metric_F1score: 0.96 - ETA: 4s - loss: 0.1687 - acc: 0.9642 - metric_F1score: 0.96 - ETA: 3s - loss: 0.1696 - acc: 0.9636 - metric_F1score: 0.96 - ETA: 3s - loss: 0.1696 - acc: 0.9628 - metric_F1score: 0.95 - ETA: 3s - loss: 0.1697 - acc: 0.9625 - metric_F1score: 0.95 - ETA: 2s - loss: 0.1711 - acc: 0.9616 - metric_F1score: 0.95 - ETA: 2s - loss: 0.1716 - acc: 0.9612 - metric_F1score: 0.95 - ETA: 2s - loss: 0.1724 - acc: 0.9605 - metric_F1score: 0.95 - ETA: 2s - loss: 0.1724 - acc: 0.9605 - metric_F1score: 0.95 - ETA: 1s - loss: 0.1726 - acc: 0.9605 - metric_F1score: 0.95 - ETA: 1s - loss: 0.1722 - acc: 0.9605 - metric_F1score: 0.95 - ETA: 1s - loss: 0.1719 - acc: 0.9602 - metric_F1score: 0.95 - ETA: 0s - loss: 0.1719 - acc: 0.9598 - metric_F1score: 0.95 - ETA: 0s - loss: 0.1714 - acc: 0.9604 - metric_F1score: 0.95 - ETA: 0s - loss: 0.1722 - acc: 0.9604 - metric_F1score: 0.95 - ETA: 0s - loss: 0.1720 - acc: 0.9604 - metric_F1score: 0.95 - 13s 2ms/step - loss: 0.1719 - acc: 0.9606 - metric_F1score: 0.9585\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5812/5812 [==============================] - ETA: 12s - loss: 0.0955 - acc: 0.9922 - metric_F1score: 0.988 - ETA: 11s - loss: 0.0940 - acc: 0.9961 - metric_F1score: 0.994 - ETA: 11s - loss: 0.0995 - acc: 0.9922 - metric_F1score: 0.992 - ETA: 11s - loss: 0.0986 - acc: 0.9922 - metric_F1score: 0.992 - ETA: 11s - loss: 0.0986 - acc: 0.9906 - metric_F1score: 0.991 - ETA: 10s - loss: 0.0982 - acc: 0.9896 - metric_F1score: 0.990 - ETA: 10s - loss: 0.1007 - acc: 0.9877 - metric_F1score: 0.988 - ETA: 10s - loss: 0.0999 - acc: 0.9873 - metric_F1score: 0.988 - ETA: 10s - loss: 0.1011 - acc: 0.9861 - metric_F1score: 0.986 - ETA: 9s - loss: 0.0984 - acc: 0.9875 - metric_F1score: 0.988 - ETA: 9s - loss: 0.0994 - acc: 0.9879 - metric_F1score: 0.98 - ETA: 9s - loss: 0.0994 - acc: 0.9883 - metric_F1score: 0.98 - ETA: 8s - loss: 0.0997 - acc: 0.9886 - metric_F1score: 0.98 - ETA: 8s - loss: 0.0988 - acc: 0.9894 - metric_F1score: 0.98 - ETA: 8s - loss: 0.0988 - acc: 0.9891 - metric_F1score: 0.98 - ETA: 8s - loss: 0.0991 - acc: 0.9897 - metric_F1score: 0.98 - ETA: 7s - loss: 0.0986 - acc: 0.9903 - metric_F1score: 0.98 - ETA: 7s - loss: 0.0998 - acc: 0.9900 - metric_F1score: 0.98 - ETA: 7s - loss: 0.1004 - acc: 0.9893 - metric_F1score: 0.98 - ETA: 7s - loss: 0.1026 - acc: 0.9867 - metric_F1score: 0.98 - ETA: 6s - loss: 0.1014 - acc: 0.9866 - metric_F1score: 0.98 - ETA: 6s - loss: 0.1009 - acc: 0.9865 - metric_F1score: 0.98 - ETA: 6s - loss: 0.1030 - acc: 0.9857 - metric_F1score: 0.98 - ETA: 6s - loss: 0.1018 - acc: 0.9860 - metric_F1score: 0.98 - ETA: 5s - loss: 0.1015 - acc: 0.9859 - metric_F1score: 0.98 - ETA: 5s - loss: 0.1032 - acc: 0.9847 - metric_F1score: 0.98 - ETA: 5s - loss: 0.1037 - acc: 0.9841 - metric_F1score: 0.98 - ETA: 4s - loss: 0.1020 - acc: 0.9847 - metric_F1score: 0.98 - ETA: 4s - loss: 0.1029 - acc: 0.9844 - metric_F1score: 0.98 - ETA: 4s - loss: 0.1029 - acc: 0.9841 - metric_F1score: 0.98 - ETA: 4s - loss: 0.1032 - acc: 0.9839 - metric_F1score: 0.98 - ETA: 3s - loss: 0.1028 - acc: 0.9841 - metric_F1score: 0.98 - ETA: 3s - loss: 0.1030 - acc: 0.9839 - metric_F1score: 0.98 - ETA: 3s - loss: 0.1041 - acc: 0.9832 - metric_F1score: 0.98 - ETA: 2s - loss: 0.1040 - acc: 0.9833 - metric_F1score: 0.98 - ETA: 2s - loss: 0.1041 - acc: 0.9833 - metric_F1score: 0.98 - ETA: 2s - loss: 0.1044 - acc: 0.9833 - metric_F1score: 0.98 - ETA: 2s - loss: 0.1042 - acc: 0.9836 - metric_F1score: 0.98 - ETA: 1s - loss: 0.1048 - acc: 0.9830 - metric_F1score: 0.98 - ETA: 1s - loss: 0.1050 - acc: 0.9828 - metric_F1score: 0.98 - ETA: 1s - loss: 0.1056 - acc: 0.9825 - metric_F1score: 0.98 - ETA: 0s - loss: 0.1052 - acc: 0.9825 - metric_F1score: 0.98 - ETA: 0s - loss: 0.1060 - acc: 0.9822 - metric_F1score: 0.98 - ETA: 0s - loss: 0.1061 - acc: 0.9821 - metric_F1score: 0.98 - ETA: 0s - loss: 0.1075 - acc: 0.9814 - metric_F1score: 0.98 - 13s 2ms/step - loss: 0.1083 - acc: 0.9812 - metric_F1score: 0.9813\n",
      "Epoch 6/10\n",
      "5812/5812 [==============================] - ETA: 11s - loss: 0.0759 - acc: 0.9844 - metric_F1score: 0.988 - ETA: 11s - loss: 0.0728 - acc: 0.9922 - metric_F1score: 0.994 - ETA: 11s - loss: 0.0754 - acc: 0.9948 - metric_F1score: 0.996 - ETA: 11s - loss: 0.0761 - acc: 0.9922 - metric_F1score: 0.993 - ETA: 11s - loss: 0.0821 - acc: 0.9859 - metric_F1score: 0.986 - ETA: 10s - loss: 0.0823 - acc: 0.9870 - metric_F1score: 0.988 - ETA: 10s - loss: 0.0845 - acc: 0.9866 - metric_F1score: 0.987 - ETA: 10s - loss: 0.0864 - acc: 0.9863 - metric_F1score: 0.987 - ETA: 10s - loss: 0.0874 - acc: 0.9861 - metric_F1score: 0.986 - ETA: 9s - loss: 0.0874 - acc: 0.9852 - metric_F1score: 0.985 - ETA: 9s - loss: 0.0863 - acc: 0.9858 - metric_F1score: 0.98 - ETA: 9s - loss: 0.0838 - acc: 0.9863 - metric_F1score: 0.98 - ETA: 9s - loss: 0.0824 - acc: 0.9868 - metric_F1score: 0.98 - ETA: 8s - loss: 0.0816 - acc: 0.9877 - metric_F1score: 0.98 - ETA: 8s - loss: 0.0812 - acc: 0.9880 - metric_F1score: 0.98 - ETA: 8s - loss: 0.0805 - acc: 0.9883 - metric_F1score: 0.98 - ETA: 8s - loss: 0.0815 - acc: 0.9876 - metric_F1score: 0.98 - ETA: 7s - loss: 0.0801 - acc: 0.9883 - metric_F1score: 0.98 - ETA: 7s - loss: 0.0787 - acc: 0.9889 - metric_F1score: 0.98 - ETA: 7s - loss: 0.0786 - acc: 0.9887 - metric_F1score: 0.98 - ETA: 6s - loss: 0.0783 - acc: 0.9888 - metric_F1score: 0.98 - ETA: 6s - loss: 0.0778 - acc: 0.9893 - metric_F1score: 0.98 - ETA: 6s - loss: 0.0769 - acc: 0.9898 - metric_F1score: 0.98 - ETA: 5s - loss: 0.0773 - acc: 0.9893 - metric_F1score: 0.98 - ETA: 5s - loss: 0.0766 - acc: 0.9894 - metric_F1score: 0.98 - ETA: 5s - loss: 0.0761 - acc: 0.9895 - metric_F1score: 0.98 - ETA: 5s - loss: 0.0755 - acc: 0.9896 - metric_F1score: 0.98 - ETA: 4s - loss: 0.0751 - acc: 0.9897 - metric_F1score: 0.98 - ETA: 4s - loss: 0.0749 - acc: 0.9895 - metric_F1score: 0.98 - ETA: 4s - loss: 0.0742 - acc: 0.9898 - metric_F1score: 0.98 - ETA: 4s - loss: 0.0736 - acc: 0.9902 - metric_F1score: 0.98 - ETA: 3s - loss: 0.0753 - acc: 0.9900 - metric_F1score: 0.98 - ETA: 3s - loss: 0.0760 - acc: 0.9898 - metric_F1score: 0.98 - ETA: 3s - loss: 0.0765 - acc: 0.9894 - metric_F1score: 0.98 - ETA: 2s - loss: 0.0765 - acc: 0.9893 - metric_F1score: 0.98 - ETA: 2s - loss: 0.0772 - acc: 0.9887 - metric_F1score: 0.98 - ETA: 2s - loss: 0.0770 - acc: 0.9888 - metric_F1score: 0.98 - ETA: 2s - loss: 0.0765 - acc: 0.9887 - metric_F1score: 0.98 - ETA: 1s - loss: 0.0768 - acc: 0.9886 - metric_F1score: 0.98 - ETA: 1s - loss: 0.0762 - acc: 0.9889 - metric_F1score: 0.98 - ETA: 1s - loss: 0.0760 - acc: 0.9891 - metric_F1score: 0.98 - ETA: 0s - loss: 0.0760 - acc: 0.9887 - metric_F1score: 0.98 - ETA: 0s - loss: 0.0755 - acc: 0.9887 - metric_F1score: 0.98 - ETA: 0s - loss: 0.0752 - acc: 0.9890 - metric_F1score: 0.98 - ETA: 0s - loss: 0.0752 - acc: 0.9891 - metric_F1score: 0.98 - 13s 2ms/step - loss: 0.0749 - acc: 0.9892 - metric_F1score: 0.9885\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5812/5812 [==============================] - ETA: 11s - loss: 0.0631 - acc: 0.9844 - metric_F1score: 0.984 - ETA: 11s - loss: 0.0584 - acc: 0.9883 - metric_F1score: 0.986 - ETA: 11s - loss: 0.0555 - acc: 0.9922 - metric_F1score: 0.989 - ETA: 11s - loss: 0.0526 - acc: 0.9922 - metric_F1score: 0.990 - ETA: 11s - loss: 0.0546 - acc: 0.9906 - metric_F1score: 0.989 - ETA: 11s - loss: 0.0550 - acc: 0.9896 - metric_F1score: 0.988 - ETA: 11s - loss: 0.0526 - acc: 0.9911 - metric_F1score: 0.989 - ETA: 10s - loss: 0.0502 - acc: 0.9922 - metric_F1score: 0.991 - ETA: 10s - loss: 0.0492 - acc: 0.9931 - metric_F1score: 0.992 - ETA: 10s - loss: 0.0480 - acc: 0.9938 - metric_F1score: 0.993 - ETA: 9s - loss: 0.0490 - acc: 0.9936 - metric_F1score: 0.992 - ETA: 9s - loss: 0.0485 - acc: 0.9941 - metric_F1score: 0.99 - ETA: 9s - loss: 0.0474 - acc: 0.9946 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0492 - acc: 0.9944 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0490 - acc: 0.9943 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0497 - acc: 0.9941 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0512 - acc: 0.9931 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0504 - acc: 0.9931 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0495 - acc: 0.9934 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0493 - acc: 0.9938 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0487 - acc: 0.9940 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0491 - acc: 0.9936 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0491 - acc: 0.9935 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0490 - acc: 0.9935 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0488 - acc: 0.9938 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0489 - acc: 0.9937 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0482 - acc: 0.9939 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0487 - acc: 0.9936 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0487 - acc: 0.9935 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0494 - acc: 0.9935 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0493 - acc: 0.9934 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0494 - acc: 0.9934 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0489 - acc: 0.9936 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0492 - acc: 0.9936 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0505 - acc: 0.9933 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0513 - acc: 0.9926 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0513 - acc: 0.9926 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0507 - acc: 0.9928 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0519 - acc: 0.9926 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0518 - acc: 0.9926 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0522 - acc: 0.9926 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0520 - acc: 0.9927 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0520 - acc: 0.9927 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0517 - acc: 0.9927 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0515 - acc: 0.9927 - metric_F1score: 0.99 - 13s 2ms/step - loss: 0.0513 - acc: 0.9928 - metric_F1score: 0.9927\n",
      "Epoch 8/10\n",
      "5812/5812 [==============================] - ETA: 13s - loss: 0.0303 - acc: 1.0000 - metric_F1score: 1.000 - ETA: 12s - loss: 0.0305 - acc: 1.0000 - metric_F1score: 1.000 - ETA: 12s - loss: 0.0322 - acc: 1.0000 - metric_F1score: 1.000 - ETA: 12s - loss: 0.0322 - acc: 1.0000 - metric_F1score: 1.000 - ETA: 11s - loss: 0.0342 - acc: 0.9984 - metric_F1score: 0.998 - ETA: 11s - loss: 0.0341 - acc: 0.9974 - metric_F1score: 0.998 - ETA: 11s - loss: 0.0334 - acc: 0.9978 - metric_F1score: 0.998 - ETA: 10s - loss: 0.0324 - acc: 0.9980 - metric_F1score: 0.998 - ETA: 10s - loss: 0.0333 - acc: 0.9974 - metric_F1score: 0.997 - ETA: 10s - loss: 0.0326 - acc: 0.9977 - metric_F1score: 0.997 - ETA: 10s - loss: 0.0319 - acc: 0.9979 - metric_F1score: 0.997 - ETA: 9s - loss: 0.0323 - acc: 0.9974 - metric_F1score: 0.997 - ETA: 9s - loss: 0.0369 - acc: 0.9958 - metric_F1score: 0.99 - ETA: 9s - loss: 0.0379 - acc: 0.9955 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0403 - acc: 0.9948 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0399 - acc: 0.9946 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0389 - acc: 0.9949 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0381 - acc: 0.9952 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0385 - acc: 0.9951 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0378 - acc: 0.9953 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0381 - acc: 0.9952 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0376 - acc: 0.9954 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0391 - acc: 0.9952 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0391 - acc: 0.9951 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0411 - acc: 0.9947 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0413 - acc: 0.9949 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0408 - acc: 0.9951 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0404 - acc: 0.9953 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0398 - acc: 0.9954 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0398 - acc: 0.9953 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0398 - acc: 0.9952 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0394 - acc: 0.9954 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0395 - acc: 0.9953 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0390 - acc: 0.9954 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0389 - acc: 0.9953 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0387 - acc: 0.9954 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0392 - acc: 0.9954 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0401 - acc: 0.9953 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0413 - acc: 0.9946 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0409 - acc: 0.9947 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0420 - acc: 0.9941 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0420 - acc: 0.9940 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0420 - acc: 0.9938 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0415 - acc: 0.9940 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0431 - acc: 0.9938 - metric_F1score: 0.99 - 13s 2ms/step - loss: 0.0430 - acc: 0.9938 - metric_F1score: 0.9938\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5812/5812 [==============================] - ETA: 13s - loss: 0.0306 - acc: 0.9922 - metric_F1score: 0.992 - ETA: 12s - loss: 0.0243 - acc: 0.9961 - metric_F1score: 0.996 - ETA: 11s - loss: 0.0414 - acc: 0.9922 - metric_F1score: 0.992 - ETA: 11s - loss: 0.0378 - acc: 0.9941 - metric_F1score: 0.994 - ETA: 11s - loss: 0.0370 - acc: 0.9953 - metric_F1score: 0.995 - ETA: 10s - loss: 0.0357 - acc: 0.9948 - metric_F1score: 0.995 - ETA: 10s - loss: 0.0351 - acc: 0.9944 - metric_F1score: 0.995 - ETA: 10s - loss: 0.0338 - acc: 0.9941 - metric_F1score: 0.994 - ETA: 10s - loss: 0.0317 - acc: 0.9948 - metric_F1score: 0.995 - ETA: 9s - loss: 0.0312 - acc: 0.9953 - metric_F1score: 0.995 - ETA: 9s - loss: 0.0302 - acc: 0.9957 - metric_F1score: 0.99 - ETA: 9s - loss: 0.0291 - acc: 0.9961 - metric_F1score: 0.99 - ETA: 9s - loss: 0.0287 - acc: 0.9964 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0282 - acc: 0.9967 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0310 - acc: 0.9964 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0304 - acc: 0.9966 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0301 - acc: 0.9963 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0300 - acc: 0.9965 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0304 - acc: 0.9963 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0356 - acc: 0.9957 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0357 - acc: 0.9955 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0349 - acc: 0.9957 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0350 - acc: 0.9959 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0351 - acc: 0.9958 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0368 - acc: 0.9953 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0362 - acc: 0.9955 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0359 - acc: 0.9957 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0352 - acc: 0.9958 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0347 - acc: 0.9960 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0353 - acc: 0.9958 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0349 - acc: 0.9960 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0347 - acc: 0.9961 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0345 - acc: 0.9962 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0345 - acc: 0.9963 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0342 - acc: 0.9964 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0337 - acc: 0.9965 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0336 - acc: 0.9964 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0340 - acc: 0.9963 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0347 - acc: 0.9962 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0348 - acc: 0.9959 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0348 - acc: 0.9958 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0354 - acc: 0.9953 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0352 - acc: 0.9955 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0348 - acc: 0.9956 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0344 - acc: 0.9957 - metric_F1score: 0.99 - 13s 2ms/step - loss: 0.0345 - acc: 0.9955 - metric_F1score: 0.9956\n",
      "Epoch 10/10\n",
      "5812/5812 [==============================] - ETA: 12s - loss: 0.0182 - acc: 1.0000 - metric_F1score: 1.000 - ETA: 12s - loss: 0.0186 - acc: 1.0000 - metric_F1score: 1.000 - ETA: 11s - loss: 0.0232 - acc: 0.9948 - metric_F1score: 0.994 - ETA: 12s - loss: 0.0222 - acc: 0.9961 - metric_F1score: 0.996 - ETA: 12s - loss: 0.0230 - acc: 0.9953 - metric_F1score: 0.996 - ETA: 11s - loss: 0.0219 - acc: 0.9961 - metric_F1score: 0.996 - ETA: 11s - loss: 0.0209 - acc: 0.9967 - metric_F1score: 0.997 - ETA: 10s - loss: 0.0220 - acc: 0.9961 - metric_F1score: 0.996 - ETA: 10s - loss: 0.0239 - acc: 0.9957 - metric_F1score: 0.996 - ETA: 10s - loss: 0.0267 - acc: 0.9953 - metric_F1score: 0.995 - ETA: 10s - loss: 0.0256 - acc: 0.9957 - metric_F1score: 0.996 - ETA: 9s - loss: 0.0250 - acc: 0.9961 - metric_F1score: 0.996 - ETA: 9s - loss: 0.0243 - acc: 0.9964 - metric_F1score: 0.99 - ETA: 9s - loss: 0.0235 - acc: 0.9967 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0290 - acc: 0.9953 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0283 - acc: 0.9956 - metric_F1score: 0.99 - ETA: 8s - loss: 0.0274 - acc: 0.9959 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0268 - acc: 0.9961 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0268 - acc: 0.9959 - metric_F1score: 0.99 - ETA: 7s - loss: 0.0265 - acc: 0.9961 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0262 - acc: 0.9963 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0260 - acc: 0.9964 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0267 - acc: 0.9959 - metric_F1score: 0.99 - ETA: 6s - loss: 0.0264 - acc: 0.9961 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0258 - acc: 0.9962 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0265 - acc: 0.9961 - metric_F1score: 0.99 - ETA: 5s - loss: 0.0264 - acc: 0.9962 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0258 - acc: 0.9964 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0268 - acc: 0.9960 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0286 - acc: 0.9958 - metric_F1score: 0.99 - ETA: 4s - loss: 0.0281 - acc: 0.9960 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0287 - acc: 0.9956 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0296 - acc: 0.9955 - metric_F1score: 0.99 - ETA: 3s - loss: 0.0300 - acc: 0.9954 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0314 - acc: 0.9951 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0323 - acc: 0.9950 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0319 - acc: 0.9951 - metric_F1score: 0.99 - ETA: 2s - loss: 0.0331 - acc: 0.9951 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0328 - acc: 0.9952 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0336 - acc: 0.9951 - metric_F1score: 0.99 - ETA: 1s - loss: 0.0335 - acc: 0.9950 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0332 - acc: 0.9952 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0328 - acc: 0.9951 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0331 - acc: 0.9950 - metric_F1score: 0.99 - ETA: 0s - loss: 0.0330 - acc: 0.9950 - metric_F1score: 0.99 - 13s 2ms/step - loss: 0.0332 - acc: 0.9948 - metric_F1score: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2106a0d3d30>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X,\n",
    "          train_Y,\n",
    "          batch_size=128,\n",
    "          epochs=10)\n",
    "# model.fit(traintitle,\n",
    "#           labels,\n",
    "#           batch_size=128,\n",
    "#           epochs=3,\n",
    "#           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6516683551697048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "pred_val = model.predict(val_X)\n",
    "print(f1_score(val_Y, np.argmax(pred_val, axis=1), average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(model.predict(testtitle), axis=1)\n",
    "test['label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test.label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id', 'label']].to_csv('baseline4.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
